{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom = 1\n",
    "image_width = 128\n",
    "image_height = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(rle_image):\n",
    "    image_array = np.zeros(image_width*image_height)\n",
    "    for rle_tuple in rle_image:\n",
    "        number_of_pixels = int(rle_tuple[1]) - 1\n",
    "        pixel = int(rle_tuple[0])\n",
    "        image_array[pixel:pixel + number_of_pixels] = 1\n",
    "#         for i in range(number_of_pixels):\n",
    "#             pixel = int(pixel)\n",
    "#             image_array[pixel + i] = 1\n",
    "    return np.reshape(image_array, (image_width, image_height), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ships_frame = pd.read_csv('data/train_ship_segmentations_v2.csv')\n",
    "\n",
    "n = 3\n",
    "img_name = ships_frame.iloc[n, 0]\n",
    "ships = ships_frame.iloc[n, 1].split(' ')\n",
    "print(ships)\n",
    "ships = np.asarray(ships).reshape(-1, 2)\n",
    "ships = rle_decode(ships)\n",
    "\n",
    "print('Image name: {}'.format(img_name))\n",
    "print('Landmarks shape: {}'.format(ships.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(ships[:4])\n",
    "print(rle_decode(ships[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ship_images(image, ships):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(ships, 'gray')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "plt.figure()\n",
    "print(img_name)\n",
    "show_ship_images(io.imread(os.path.join('data/train_v2', img_name)),\n",
    "               ships)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __init__(self, dimensions=None):\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, loc_image = sample['image'], sample['loc_image']\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        if self.dimensions:\n",
    "            height, width = self.dimensions\n",
    "            image = transform.resize(image, (height, width), anti_aliasing=True, preserve_range=True)\n",
    "            loc_image = transform.resize(loc_image, (height, width), anti_aliasing=False, preserve_range=True)\n",
    "            loc_image[loc_image > 0] = 255\n",
    "            \n",
    "        image = image.transpose((2, 0, 1)).astype(np.float32) / 255\n",
    "        loc_image = loc_image.reshape(1, loc_image.shape[0], loc_image.shape[1]).astype(np.float32) / 255\n",
    "        contains_ship = np.asarray([1, 0], dtype=np.float32) if np.count_nonzero(loc_image) == 0 else np.asarray([0,1],dtype=np.float32)\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'loc_image': torch.from_numpy(loc_image),\n",
    "                'contains_ship': torch.from_numpy(contains_ship)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ships_dataframe = pd.read_csv('data/train_ship_segmentations_v2.csv')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ships_dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.ships_dataframe.iloc[idx, 0]\n",
    "        image_path = 'data/train_v2/' + image_name\n",
    "        image = io.imread(image_path)\n",
    "        if pd.isnull(self.ships_dataframe.iloc[idx, 1]):\n",
    "            ship_loc_data = []\n",
    "        else:\n",
    "            ship_loc_data = self.ships_dataframe.iloc[idx, 1]\n",
    "            ship_loc_data = ship_loc_data.split(' ')\n",
    "            ship_loc_data = np.asarray(ship_loc_data).reshape(-1, 2)\n",
    "        \n",
    "        loc_image = rle_decode(ship_loc_data)\n",
    "        \n",
    "        sample = {'image': image, 'loc_image': loc_image}\n",
    "        return sample\n",
    "                \n",
    "\n",
    "class MergeShipData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ships_dataframe = pd.read_csv('data/train_ship_segmentations_v2.csv')\n",
    "        self.merge_ships_dict = self._merge_ships(self.ships_dataframe)\n",
    "        self.merge_ships_dataframe = self.ships_dataframe.drop('EncodedPixels', 1).drop_duplicates()\n",
    "        self.transform = transforms.Compose([ToTensor((image_height, image_width))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.merge_ships_dict) - 1\n",
    "    \n",
    "    def _get_blank_sample(self):\n",
    "        image = np.zeros((image_height, image_width, 3))\n",
    "        loc_image = rle_decode([])\n",
    "        return {\"image\": image, \"loc_image\": loc_image, 'contains_ship': 0}\n",
    "    \n",
    "    def _get_image_mask(self, idx):\n",
    "        image_name = self.ships_dataframe.iloc[idx, 0]\n",
    "        if pd.isnull(self.ships_dataframe.iloc[idx, 1]):\n",
    "            ship_loc_data = []\n",
    "        else:\n",
    "            ship_loc_data = self.ships_dataframe.iloc[idx, 1]\n",
    "            ship_loc_data = ship_loc_data.split(' ')\n",
    "            ship_loc_data = np.asarray(ship_loc_data).reshape(-1, 2)\n",
    "        \n",
    "        loc_image = rle_decode(ship_loc_data)\n",
    "        return loc_image\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image_name = self.merge_ships_dataframe.iloc[idx, 0]\n",
    "            image_idxs = self.merge_ships_dict[image_name]\n",
    "            image_path = 'data/train_v2/' + image_name\n",
    "            image = io.imread(image_path)\n",
    "            loc_image = self._merge_images(image_idxs)\n",
    "            sample = {'image': image, 'loc_image': loc_image }\n",
    "        except:\n",
    "            sample = self._get_blank_sample()\n",
    "        return self.transform(sample)\n",
    "    \n",
    "    def _merge_ships(self, df):\n",
    "        ships = {}\n",
    "        for idx, row in df.iterrows():\n",
    "            image_name = row[0]\n",
    "            if image_name in ships:\n",
    "                ships[image_name].append(idx)\n",
    "            else:\n",
    "                ships[image_name] = [idx]\n",
    "        return ships\n",
    "    \n",
    "    def _merge_images(self, idxs):\n",
    "        img = rle_decode([])\n",
    "        for idx in idxs:\n",
    "            img = np.logical_or(img, self._get_image_mask(idx)).astype(float)\n",
    "\n",
    "        return img.astype(np.uint8) * 255\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = MergeShipData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last = 0\n",
    "# for i in range(len(sd)):\n",
    "#     last = i\n",
    "#     print(i)\n",
    "#     sd[i]\n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(sample):\n",
    "    toPilImage = transforms.ToPILImage()\n",
    "    to1ChannelPilImage = transforms.ToPILImage()\n",
    "    image = toPilImage(sample['image'])\n",
    "    loc_image = to1ChannelPilImage(sample['loc_image'])\n",
    "    plt.figure(num=None, figsize=(12, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(loc_image, 'gray')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to = 40\n",
    "number_from = 0\n",
    "for i in range(number_from, number_to):\n",
    "    plot_sample(sd[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_and_test_idxs(dataset_length, train_split):\n",
    "    np.random.seed(42)\n",
    "    idxs = list(range(dataset_length))\n",
    "    split = int(np.floor(train_split * dataset_length))\n",
    "    np.random.shuffle(idxs)\n",
    "    return idxs[:split], idxs[split:]\n",
    "\n",
    "def get_train_and_test_datasets(dataset, max_length=None, batch_size=8):\n",
    "    dataset_length = max_length or len(dataset)\n",
    "    train_indices, val_indices = generate_train_and_test_idxs(dataset_length, train_split)\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    validation_sampler = SubsetRandomSampler(val_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                    sampler=validation_sampler)\n",
    "    return train_loader, validation_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read https://arxiv.org/pdf/1505.04366.pdf\n",
    "# perhaps combine with https://arxiv.org/pdf/1311.2524.pdf\n",
    "class MaskModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskModel, self).__init__()\n",
    "        \n",
    "        # conv layers\n",
    "        self.cnn1 = torch.nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.cnn2 = torch.nn.Conv2d(16, 32, 3, padding=1)        \n",
    "        self.cnn3 = torch.nn.Conv2d(32, 64, 3, padding=1)        \n",
    "        self.cnn4 = torch.nn.Conv2d(64, 128, 3, padding=1)\n",
    "\n",
    "        \n",
    "        # deconv layers\n",
    "        self.decnn1 = torch.nn.ConvTranspose2d(128, 64, 3, padding=1)\n",
    "        self.decnn2 = torch.nn.ConvTranspose2d(64, 32, 3, padding=1)\n",
    "        self.decnn3 = torch.nn.ConvTranspose2d(32, 16, 3, padding=1)\n",
    "        self.decnn4 = torch.nn.ConvTranspose2d(16, 1, 3, padding=1)\n",
    "        \n",
    "        \n",
    "        # pooling layers\n",
    "        self.pool2d = torch.nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        self.unpool2d = torch.nn.MaxUnpool2d(2, stride=2)\n",
    "        \n",
    "        # activation layers\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.softmax = torch.nn.Softmax2d()\n",
    "        \n",
    "        # other layers\n",
    "        self.dropout = torch.nn.Dropout2d()\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(16)        \n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(32)        \n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(64)        \n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(128)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = self.cnn1(x)\n",
    "        result = self.batchnorm1(result)\n",
    "        result = self.relu(result)\n",
    "        result, indices1 = self.pool2d(result)\n",
    "        result = self.cnn2(result)\n",
    "        result = self.batchnorm2(result)\n",
    "        result = self.relu(result)\n",
    "        result, indices2 = self.pool2d(result)\n",
    "        result = self.cnn3(result)\n",
    "        result = self.batchnorm3(result)\n",
    "        result = self.relu(result)\n",
    "        result, indices3 = self.pool2d(result)\n",
    "        result = self.cnn4(result)\n",
    "        result = self.batchnorm4(result)\n",
    "        result = self.relu(result)\n",
    "        result, indices4 = self.pool2d(result)\n",
    "        \n",
    "        result = self.unpool2d(result, indices4)\n",
    "        result = self.decnn1(result)\n",
    "        result = self.batchnorm3(result)\n",
    "        result = self.relu(result)\n",
    "        result = self.unpool2d(result, indices3)\n",
    "        result = self.decnn2(result)\n",
    "        result = self.batchnorm2(result)\n",
    "        result = self.relu(result)\n",
    "        result = self.unpool2d(result, indices2)\n",
    "        result = self.decnn3(result)\n",
    "        result = self.batchnorm1(result)\n",
    "        result = self.relu(result)\n",
    "        result = self.unpool2d(result, indices1)\n",
    "        result = self.decnn4(result)\n",
    "        result = self.sigmoid(result)\n",
    "        return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_categorization_model():\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(3, 16, 3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(2, stride=2),\n",
    "        torch.nn.Conv2d(16, 20 , 3),\n",
    "        torch.nn.Dropout2d(),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(2, stride=2),\n",
    "#         torch.nn.Conv2d(64, 128, 3, padding=1),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.MaxPool2d(2, stride=2),\n",
    "#         torch.nn.Conv2d(128, 256, 3, padding=1),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.MaxPool2d(2, stride=2),\n",
    "#         torch.nn.Conv2d(256, 128, 3, padding=1),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.MaxPool2d(2, stride=2),\n",
    "#         torch.nn.Conv2d(128, 256, 3, padding=1),\n",
    "#         torch.nn.ReLU(),\n",
    "#         torch.nn.MaxPool2d(2, stride=2),\n",
    "        Flatten(),\n",
    "        torch.nn.Linear((94*94*20), 1024),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(1024, 512),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(512, 2),\n",
    "        torch.nn.Softmax()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validation_loader = get_train_and_test_datasets(sd, max_length=1000000, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Train:   \n",
    "    for batch_index, sample in enumerate(train_loader):\n",
    "        \n",
    "        image = sample['image'].to(device)\n",
    "        labels = sample['loc_image'].to(device)\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, labels.view(32, 128, -1))\n",
    "        loss_history.append(loss.item())\n",
    "        plt.plot(loss_history, color='blue')\n",
    "        plt.ylabel('loss')\n",
    "        #plt.show()\n",
    "        #plt.plot(loss_history[len(loss_history) - 24:])\n",
    "        #plt.ylabel('loss in last 24 batches')\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "        print(f'loss: {loss.item()}\\nbatch: {batch_index}\\nepoch: {epoch}')\n",
    "        \n",
    "        \n",
    "        #display(plt.gcf())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for index, sample in enumerate(validation_loader):\n",
    "        image = sample['image'].to(device)\n",
    "        labels = sample['contains_ship'].to(device)\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu().data.numpy().astype(np.float32) == labels[:,0]).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPilImage = transforms.ToPILImage()\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "image = sd[50]['image'].to(device)\n",
    "output = model(torch.unsqueeze(image, 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPilImage((output * (255/(output.max()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPilImage(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_and_compare_result(model, sample):\n",
    "    model.eval()\n",
    "    image = sample['image'].to(device)\n",
    "    label = sample['loc_image'].to(device)\n",
    "    output = model(torch.unsqueeze(image, 0))[0]\n",
    "    \n",
    "    \n",
    "    toPilImage = transforms.ToPILImage()\n",
    "    image = toPilImage(sample['image'])\n",
    "    loc_image = toPilImage(sample['loc_image'])\n",
    "    output_image = toPilImage((output * 255/output.max()))\n",
    "    plt.figure(num=None, figsize=(32, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(loc_image, 'gray')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(output_image, 'gray')\n",
    "    \n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sd[50]['loc_image'].max())\n",
    "infer_and_compare_result(model, sd[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
